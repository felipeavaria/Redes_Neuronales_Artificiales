{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 3:\n",
    "## Predicción del Precio de una Casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Construcción del dataframe, el dataset contiene 14 atributos para describir propiedades, de estos 14 atributos el ultimo es el que desamos predecir (MEDV, valor medio de las viviendas).\n",
    "\n",
    "atributos:\n",
    "1. CRIM: tasa de criminalidad per cápita por municipio \n",
    "2. ZN: proporción de suelo residencial dividido en zonas para las porciones más de 25.000 pies cuadrados \n",
    "3. INDUS: proporción de acres de negocios no minoristas por ciudad \n",
    "4. CHAS: Charles River variable ficticia (= 1 si sale del tracto río; 0 en caso contrario) \n",
    "5. NOX: la concentración de óxidos de nitrógeno (partes por 10 millones) \n",
    "6. RM: promedio número de habitaciones por vivienda \n",
    "7. EDAD: proporción de unidades ocupadas por el propietario construyó antes de 1940 \n",
    "8. DIS: distancias ponderadas a cinco centros de empleo de Boston \n",
    "9. RAD: índice de accesibilidad a las autopistas radiales \n",
    "10. iMPUESTOS: valor total de impuestos sobre bienes tasa por $ 10,000 \n",
    "11. PTRATIO: alumno-maestro por ciudad \n",
    "12. B: 1000 (Bk - 0,63) ^ 2 donde Bk es la proporción de los negros por ciudad \n",
    "13. LSTAT:% de estado inferior de la población \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Archivo CSV\n",
    "url = 'DatasetCasa.csv'\n",
    "#lectura\n",
    "df = pd.read_csv(url, sep=' ',header=None, names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',\n",
    "'RM', 'AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'])\n",
    "#separación entre datos de entrenemiento y datos de prueba\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# 25% datos de prueba\n",
    "df_train,df_test= train_test_split(df,test_size=0.25, random_state=0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalización de los datos, esto es conveniente debido a que las magnitudes de los atributos son muy variadas y de esta misma forma al inter-relacionar las dimensiones estas no arrastran unidades de medición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "scaler = StandardScaler().fit(df_test)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "\n",
    "y_train = df_train.pop('MEDV')\n",
    "y_test = df_test.pop('MEDV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/.local/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_initializer=\"uniform\", input_dim=14)`\n",
      "  \n",
      "/home/felipe/.local/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 120.8218 - val_loss: 45.5070\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 32.0872 - val_loss: 46.1576\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 23.7724 - val_loss: 32.3389\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 17.1716 - val_loss: 23.0944\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 14.5977 - val_loss: 19.9984\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 12.1233 - val_loss: 17.5738\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 10.8229 - val_loss: 15.1424\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 9.6897 - val_loss: 14.4068\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 8.3714 - val_loss: 12.3353\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 8.0446 - val_loss: 10.9867\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 6.9683 - val_loss: 10.2047\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 6.5365 - val_loss: 10.1694\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 6.4976 - val_loss: 8.6602\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 5.7364 - val_loss: 8.4348\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 5.2846 - val_loss: 7.7999\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 5.0778 - val_loss: 6.4355\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 4.9041 - val_loss: 6.6496\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 4.3923 - val_loss: 5.3701\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 3.8530 - val_loss: 4.7354\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 3.6223 - val_loss: 4.6544\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 3.1550 - val_loss: 4.0245\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - 0s - loss: 2.9619 - val_loss: 3.5893\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 2.6237 - val_loss: 3.2151\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 2.4851 - val_loss: 3.2627\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 2.2282 - val_loss: 3.1448\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 2.0687 - val_loss: 2.5413\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 1.7876 - val_loss: 2.1412\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 1.6550 - val_loss: 1.9088\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 1.5341 - val_loss: 1.8159\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 1.5363 - val_loss: 1.7201\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 1.2284 - val_loss: 1.4010\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - 0s - loss: 1.1320 - val_loss: 1.2650\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 1.0899 - val_loss: 1.1186\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.9246 - val_loss: 1.0310\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.8517 - val_loss: 1.0984\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.7895 - val_loss: 1.1049\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.7017 - val_loss: 0.9974\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.6679 - val_loss: 0.7036\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.5941 - val_loss: 0.7058\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 0.5413 - val_loss: 0.6148\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.5212 - val_loss: 0.6425\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.5164 - val_loss: 0.5168\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.4707 - val_loss: 0.5071\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.4342 - val_loss: 0.5446\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.4154 - val_loss: 0.4413\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.3721 - val_loss: 0.4280\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.3795 - val_loss: 0.4013\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.3626 - val_loss: 0.4031\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.3411 - val_loss: 0.3629\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.3300 - val_loss: 0.4463\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.3158 - val_loss: 0.4057\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.2994 - val_loss: 0.4651\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.2948 - val_loss: 0.3603\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.2810 - val_loss: 0.3545\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.2812 - val_loss: 0.2935\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.3010 - val_loss: 0.3456\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.2788 - val_loss: 0.3078\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.2762 - val_loss: 0.4423\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.2596 - val_loss: 0.3270\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.2553 - val_loss: 0.3726\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.2454 - val_loss: 0.3131\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.2518 - val_loss: 0.2678\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.2425 - val_loss: 0.3357\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.2425 - val_loss: 0.2761\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.2373 - val_loss: 0.2443\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.2348 - val_loss: 0.2806\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.2269 - val_loss: 0.2419\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.2183 - val_loss: 0.2614\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.2279 - val_loss: 0.2465\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.2208 - val_loss: 0.2878\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.2215 - val_loss: 0.3375\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.2159 - val_loss: 0.4044\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.2163 - val_loss: 0.3218\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.2090 - val_loss: 0.3008\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.2113 - val_loss: 0.3511\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.2230 - val_loss: 0.2547\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.2112 - val_loss: 0.3590\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.2122 - val_loss: 0.3117\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.2050 - val_loss: 0.2570\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.2181 - val_loss: 0.2275\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.2017 - val_loss: 0.2146\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.2006 - val_loss: 0.3588\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.2102 - val_loss: 0.2087\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.1998 - val_loss: 0.2061\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.2049 - val_loss: 0.2523\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 0.1885 - val_loss: 0.2041\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.2029 - val_loss: 0.1947\n",
      "Epoch 88/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.1884 - val_loss: 0.2168\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.1925 - val_loss: 0.2200\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.1837 - val_loss: 0.2540\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.1817 - val_loss: 0.2094\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.1819 - val_loss: 0.2629\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.1859 - val_loss: 0.1962\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.1840 - val_loss: 0.2248\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 0.1744 - val_loss: 0.3046\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.1731 - val_loss: 0.2059\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.1858 - val_loss: 0.2045\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.1704 - val_loss: 0.2256\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.2026 - val_loss: 0.2033\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.1799 - val_loss: 0.2897\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.1732 - val_loss: 0.2522\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.1787 - val_loss: 0.1808\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.1738 - val_loss: 0.1821\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.1826 - val_loss: 0.2404\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.1703 - val_loss: 0.1927\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.1639 - val_loss: 0.2368\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.1659 - val_loss: 0.1825\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.1604 - val_loss: 0.1918\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - 0s - loss: 0.1595 - val_loss: 0.3230\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.1630 - val_loss: 0.2347\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.1656 - val_loss: 0.2331\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.1648 - val_loss: 0.2569\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.1568 - val_loss: 0.1881\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - 0s - loss: 0.1568 - val_loss: 0.2523\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.1649 - val_loss: 0.2531\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.1607 - val_loss: 0.1908\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.1624 - val_loss: 0.1872\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.1536 - val_loss: 0.2381\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.1571 - val_loss: 0.1819\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.1511 - val_loss: 0.2126\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.1512 - val_loss: 0.1637\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.1662 - val_loss: 0.2208\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.1485 - val_loss: 0.1759\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.1531 - val_loss: 0.1980\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.1623 - val_loss: 0.2302\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.1482 - val_loss: 0.1714\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.1459 - val_loss: 0.2137\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.1432 - val_loss: 0.2446\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.1543 - val_loss: 0.2003\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.1488 - val_loss: 0.1605\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.1512 - val_loss: 0.1714\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.1580 - val_loss: 0.1525\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.1435 - val_loss: 0.3067\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.1447 - val_loss: 0.2563\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.1396 - val_loss: 0.2137\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.1399 - val_loss: 0.1954\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.1451 - val_loss: 0.2815\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.1402 - val_loss: 0.2365\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.1397 - val_loss: 0.1956\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.1366 - val_loss: 0.2431\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.1392 - val_loss: 0.1775\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.1414 - val_loss: 0.2564\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.1370 - val_loss: 0.2044\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.1407 - val_loss: 0.1396\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.1314 - val_loss: 0.2196\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.1334 - val_loss: 0.2160\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.1332 - val_loss: 0.1580\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.1345 - val_loss: 0.1901\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.1316 - val_loss: 0.1758\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.1354 - val_loss: 0.1955\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.1351 - val_loss: 0.2108\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.1335 - val_loss: 0.1943\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.1254 - val_loss: 0.2189\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.1270 - val_loss: 0.1605\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.123 - 0s - loss: 0.1365 - val_loss: 0.1996\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.1264 - val_loss: 0.1993\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.1243 - val_loss: 0.2343\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - 0s - loss: 0.1281 - val_loss: 0.2227\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.1254 - val_loss: 0.1477\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.1212 - val_loss: 0.2633\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.1255 - val_loss: 0.2100\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.1215 - val_loss: 0.1972\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.1215 - val_loss: 0.3015\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.1283 - val_loss: 0.2731\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.1231 - val_loss: 0.2082\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.1298 - val_loss: 0.1925\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.1236 - val_loss: 0.2704\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.1252 - val_loss: 0.1811\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.1231 - val_loss: 0.2567\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.1235 - val_loss: 0.2684\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.1244 - val_loss: 0.1963\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.1166 - val_loss: 0.1907\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.1222 - val_loss: 0.2008\n",
      "Epoch 174/300\n",
      "379/379 [==============================] - 0s - loss: 0.1163 - val_loss: 0.1763\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.1166 - val_loss: 0.1688\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.1149 - val_loss: 0.1582\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.1159 - val_loss: 0.1660\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.1272 - val_loss: 0.1962\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.1173 - val_loss: 0.1619\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.1151 - val_loss: 0.2974\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.095 - 0s - loss: 0.1186 - val_loss: 0.1849\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.1141 - val_loss: 0.1413\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.1162 - val_loss: 0.2317\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.1234 - val_loss: 0.1702\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.1130 - val_loss: 0.1342\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.1115 - val_loss: 0.1538\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.1123 - val_loss: 0.1676\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.1170 - val_loss: 0.2012\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.1107 - val_loss: 0.2861\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.1117 - val_loss: 0.1540\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.1093 - val_loss: 0.1885\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1840\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.1073 - val_loss: 0.1591\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.1087 - val_loss: 0.1951\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.1118 - val_loss: 0.1570\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.1055 - val_loss: 0.2300\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.1089 - val_loss: 0.2023\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.1072 - val_loss: 0.1533\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.1120 - val_loss: 0.1931\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.1072 - val_loss: 0.2542\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.1064 - val_loss: 0.2024\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.1053 - val_loss: 0.1583\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.1083 - val_loss: 0.1498\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.1047 - val_loss: 0.2307\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.094 - 0s - loss: 0.1090 - val_loss: 0.1718\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.1071 - val_loss: 0.2071\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1729\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.1032 - val_loss: 0.2071\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.1021 - val_loss: 0.2456\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.1086 - val_loss: 0.1979\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.1059 - val_loss: 0.2005\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.1041 - val_loss: 0.2261\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.1082 - val_loss: 0.2013\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.1077 - val_loss: 0.1536\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.1026 - val_loss: 0.1841\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.1037 - val_loss: 0.1448\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.1023 - val_loss: 0.2123\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.1012 - val_loss: 0.1493\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.0999 - val_loss: 0.2443\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.1006 - val_loss: 0.1602\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.0990 - val_loss: 0.1598\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.0978 - val_loss: 0.1608\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.1005 - val_loss: 0.1477\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.1025 - val_loss: 0.1600\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0986 - val_loss: 0.1432\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0970 - val_loss: 0.1949\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0957 - val_loss: 0.2045\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0960 - val_loss: 0.1210\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.1036 - val_loss: 0.1287\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.0964 - val_loss: 0.1875\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0958 - val_loss: 0.1610\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0917 - val_loss: 0.2716\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.1000 - val_loss: 0.1869\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0948 - val_loss: 0.1602\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0943 - val_loss: 0.1497\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0920 - val_loss: 0.2286\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0930 - val_loss: 0.1900\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0962 - val_loss: 0.2274\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0973 - val_loss: 0.1305\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0962 - val_loss: 0.1579\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0920 - val_loss: 0.1741\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0983 - val_loss: 0.1525\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0922 - val_loss: 0.1880\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.0912 - val_loss: 0.1472\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0907 - val_loss: 0.1710\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0931 - val_loss: 0.1426\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0925 - val_loss: 0.1519\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0910 - val_loss: 0.1511\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0908 - val_loss: 0.1823\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0904 - val_loss: 0.1635\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0888 - val_loss: 0.2245\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0924 - val_loss: 0.1589\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0869 - val_loss: 0.1530\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0893 - val_loss: 0.1870\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0871 - val_loss: 0.1731\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0880 - val_loss: 0.1700\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0875 - val_loss: 0.2152\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0875 - val_loss: 0.1664\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0904 - val_loss: 0.1479\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0885 - val_loss: 0.1938\n",
      "Epoch 261/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.0894 - val_loss: 0.1612\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0868 - val_loss: 0.1863\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0912 - val_loss: 0.1321\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0860 - val_loss: 0.1631\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0880 - val_loss: 0.1995\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0893 - val_loss: 0.1821\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0860 - val_loss: 0.1304\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0884 - val_loss: 0.1417\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.0864 - val_loss: 0.1696\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0858 - val_loss: 0.1364\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0846 - val_loss: 0.1411\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0879 - val_loss: 0.1337\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.122 - 0s - loss: 0.0863 - val_loss: 0.1997\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0828 - val_loss: 0.1684\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0832 - val_loss: 0.1464\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0829 - val_loss: 0.2378\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0892 - val_loss: 0.1326\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0814 - val_loss: 0.1408\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0844 - val_loss: 0.2401\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0840 - val_loss: 0.1529\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0834 - val_loss: 0.2405\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0903 - val_loss: 0.1472\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0868 - val_loss: 0.1827\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.0802 - val_loss: 0.1776\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.0862 - val_loss: 0.1934\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0831 - val_loss: 0.1542\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0822 - val_loss: 0.1538\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0801 - val_loss: 0.2081\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0815 - val_loss: 0.1390\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1548\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0813 - val_loss: 0.1551\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0814 - val_loss: 0.1408\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0784 - val_loss: 0.1378\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0786 - val_loss: 0.1471\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0799 - val_loss: 0.1527\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0794 - val_loss: 0.2477\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0821 - val_loss: 0.1285\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0778 - val_loss: 0.1701\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0786 - val_loss: 0.1732\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0814 - val_loss: 0.1345\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcba7c65890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1, init='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "hist = model.fit(X_train_scaled.as_matrix(), y_train.as_matrix(), nb_epoch=300,\n",
    "verbose = 1, validation_data=(X_test_scaled.as_matrix(), y_test.as_matrix()))\n",
    "listaloss = []\n",
    "                \n",
    "for i in range(300):\n",
    "    listaloss.append(hist.history['loss'][i])\n",
    "\n",
    "plt.gcf().clear() #limpia lo dibujado previamente en pl\n",
    "plt.plot(range(301)[1:],listaloss,'g',label='loss')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) repetir (c) pero con relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/.local/lib/python2.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_initializer=\"uniform\", input_dim=14)`\n",
      "  \n",
      "/home/felipe/.local/lib/python2.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379 samples, validate on 127 samples\n",
      "Epoch 1/300\n",
      "379/379 [==============================] - 0s - loss: 265.7746 - val_loss: 38.7880\n",
      "Epoch 2/300\n",
      "379/379 [==============================] - 0s - loss: 15.4344 - val_loss: 13.6001\n",
      "Epoch 3/300\n",
      "379/379 [==============================] - 0s - loss: 6.4182 - val_loss: 7.6653\n",
      "Epoch 4/300\n",
      "379/379 [==============================] - 0s - loss: 3.8393 - val_loss: 4.1234\n",
      "Epoch 5/300\n",
      "379/379 [==============================] - 0s - loss: 2.2906 - val_loss: 3.3590\n",
      "Epoch 6/300\n",
      "379/379 [==============================] - 0s - loss: 1.7732 - val_loss: 3.2886\n",
      "Epoch 7/300\n",
      "379/379 [==============================] - 0s - loss: 1.3978 - val_loss: 1.9043\n",
      "Epoch 8/300\n",
      "379/379 [==============================] - 0s - loss: 1.0845 - val_loss: 1.9187\n",
      "Epoch 9/300\n",
      "379/379 [==============================] - 0s - loss: 0.9902 - val_loss: 1.2004\n",
      "Epoch 10/300\n",
      "379/379 [==============================] - 0s - loss: 0.8600 - val_loss: 1.0412\n",
      "Epoch 11/300\n",
      "379/379 [==============================] - 0s - loss: 0.7734 - val_loss: 1.7554\n",
      "Epoch 12/300\n",
      "379/379 [==============================] - 0s - loss: 1.0008 - val_loss: 2.2632\n",
      "Epoch 13/300\n",
      "379/379 [==============================] - 0s - loss: 0.8581 - val_loss: 0.8123\n",
      "Epoch 14/300\n",
      "379/379 [==============================] - 0s - loss: 1.0486 - val_loss: 0.9151\n",
      "Epoch 15/300\n",
      "379/379 [==============================] - 0s - loss: 0.6953 - val_loss: 1.1865\n",
      "Epoch 16/300\n",
      "379/379 [==============================] - 0s - loss: 0.4242 - val_loss: 0.9041\n",
      "Epoch 17/300\n",
      "379/379 [==============================] - 0s - loss: 0.3931 - val_loss: 1.1554\n",
      "Epoch 18/300\n",
      "379/379 [==============================] - 0s - loss: 0.3609 - val_loss: 0.4968\n",
      "Epoch 19/300\n",
      "379/379 [==============================] - 0s - loss: 0.4528 - val_loss: 1.1520\n",
      "Epoch 20/300\n",
      "379/379 [==============================] - 0s - loss: 0.5110 - val_loss: 0.6111\n",
      "Epoch 21/300\n",
      "379/379 [==============================] - 0s - loss: 0.3103 - val_loss: 0.4643\n",
      "Epoch 22/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.752 - 0s - loss: 1.9109 - val_loss: 1.1743\n",
      "Epoch 23/300\n",
      "379/379 [==============================] - 0s - loss: 0.4022 - val_loss: 0.5662\n",
      "Epoch 24/300\n",
      "379/379 [==============================] - 0s - loss: 0.3909 - val_loss: 0.8243\n",
      "Epoch 25/300\n",
      "379/379 [==============================] - 0s - loss: 0.2874 - val_loss: 0.9866\n",
      "Epoch 26/300\n",
      "379/379 [==============================] - 0s - loss: 0.2415 - val_loss: 0.4315\n",
      "Epoch 27/300\n",
      "379/379 [==============================] - 0s - loss: 0.2049 - val_loss: 0.6492\n",
      "Epoch 28/300\n",
      "379/379 [==============================] - 0s - loss: 0.2264 - val_loss: 0.4483\n",
      "Epoch 29/300\n",
      "379/379 [==============================] - 0s - loss: 0.2365 - val_loss: 0.7098\n",
      "Epoch 30/300\n",
      "379/379 [==============================] - 0s - loss: 0.1936 - val_loss: 0.3266\n",
      "Epoch 31/300\n",
      "379/379 [==============================] - 0s - loss: 0.2750 - val_loss: 0.9387\n",
      "Epoch 32/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.284 - 0s - loss: 1.0786 - val_loss: 1.2976\n",
      "Epoch 33/300\n",
      "379/379 [==============================] - 0s - loss: 0.4913 - val_loss: 0.6639\n",
      "Epoch 34/300\n",
      "379/379 [==============================] - 0s - loss: 0.1642 - val_loss: 0.3684\n",
      "Epoch 35/300\n",
      "379/379 [==============================] - 0s - loss: 0.1734 - val_loss: 0.4819\n",
      "Epoch 36/300\n",
      "379/379 [==============================] - 0s - loss: 0.1603 - val_loss: 0.5455\n",
      "Epoch 37/300\n",
      "379/379 [==============================] - 0s - loss: 0.2508 - val_loss: 0.5534\n",
      "Epoch 38/300\n",
      "379/379 [==============================] - 0s - loss: 0.2605 - val_loss: 0.3262\n",
      "Epoch 39/300\n",
      "379/379 [==============================] - 0s - loss: 0.4876 - val_loss: 2.5088\n",
      "Epoch 40/300\n",
      "379/379 [==============================] - 0s - loss: 1.4749 - val_loss: 1.0952\n",
      "Epoch 41/300\n",
      "379/379 [==============================] - 0s - loss: 0.2725 - val_loss: 0.5558\n",
      "Epoch 42/300\n",
      "379/379 [==============================] - 0s - loss: 0.1190 - val_loss: 0.5236\n",
      "Epoch 43/300\n",
      "379/379 [==============================] - 0s - loss: 0.2082 - val_loss: 0.8290\n",
      "Epoch 44/300\n",
      "379/379 [==============================] - 0s - loss: 0.2214 - val_loss: 0.5376\n",
      "Epoch 45/300\n",
      "379/379 [==============================] - 0s - loss: 0.1325 - val_loss: 0.5060\n",
      "Epoch 46/300\n",
      "379/379 [==============================] - 0s - loss: 0.1194 - val_loss: 0.5105\n",
      "Epoch 47/300\n",
      "379/379 [==============================] - 0s - loss: 0.0953 - val_loss: 0.3983\n",
      "Epoch 48/300\n",
      "379/379 [==============================] - 0s - loss: 0.2195 - val_loss: 0.2683\n",
      "Epoch 49/300\n",
      "379/379 [==============================] - 0s - loss: 0.0940 - val_loss: 0.3617\n",
      "Epoch 50/300\n",
      "379/379 [==============================] - 0s - loss: 0.1264 - val_loss: 0.4479\n",
      "Epoch 51/300\n",
      "379/379 [==============================] - 0s - loss: 0.0859 - val_loss: 0.6156\n",
      "Epoch 52/300\n",
      "379/379 [==============================] - 0s - loss: 0.3651 - val_loss: 1.4332\n",
      "Epoch 53/300\n",
      "379/379 [==============================] - 0s - loss: 0.5260 - val_loss: 0.3319\n",
      "Epoch 54/300\n",
      "379/379 [==============================] - 0s - loss: 0.0887 - val_loss: 0.3795\n",
      "Epoch 55/300\n",
      "379/379 [==============================] - 0s - loss: 0.0844 - val_loss: 0.3140\n",
      "Epoch 56/300\n",
      "379/379 [==============================] - 0s - loss: 0.1203 - val_loss: 0.4650\n",
      "Epoch 57/300\n",
      "379/379 [==============================] - 0s - loss: 0.4463 - val_loss: 1.1246\n",
      "Epoch 58/300\n",
      "379/379 [==============================] - 0s - loss: 0.1939 - val_loss: 0.6725\n",
      "Epoch 59/300\n",
      "379/379 [==============================] - 0s - loss: 0.2136 - val_loss: 0.4480\n",
      "Epoch 60/300\n",
      "379/379 [==============================] - 0s - loss: 0.0875 - val_loss: 0.5024\n",
      "Epoch 61/300\n",
      "379/379 [==============================] - 0s - loss: 0.0869 - val_loss: 0.2710\n",
      "Epoch 62/300\n",
      "379/379 [==============================] - 0s - loss: 0.1222 - val_loss: 0.2741\n",
      "Epoch 63/300\n",
      "379/379 [==============================] - 0s - loss: 0.1177 - val_loss: 0.4241\n",
      "Epoch 64/300\n",
      "379/379 [==============================] - 0s - loss: 0.1222 - val_loss: 0.6505\n",
      "Epoch 65/300\n",
      "379/379 [==============================] - 0s - loss: 0.1287 - val_loss: 0.3258\n",
      "Epoch 66/300\n",
      "379/379 [==============================] - 0s - loss: 0.0874 - val_loss: 0.2632\n",
      "Epoch 67/300\n",
      "379/379 [==============================] - 0s - loss: 0.0704 - val_loss: 0.2704\n",
      "Epoch 68/300\n",
      "379/379 [==============================] - 0s - loss: 0.1315 - val_loss: 0.3049\n",
      "Epoch 69/300\n",
      "379/379 [==============================] - 0s - loss: 0.2136 - val_loss: 0.6072\n",
      "Epoch 70/300\n",
      "379/379 [==============================] - 0s - loss: 0.0780 - val_loss: 0.3970\n",
      "Epoch 71/300\n",
      "379/379 [==============================] - 0s - loss: 0.0876 - val_loss: 0.2557\n",
      "Epoch 72/300\n",
      "379/379 [==============================] - 0s - loss: 0.0887 - val_loss: 0.2388\n",
      "Epoch 73/300\n",
      "379/379 [==============================] - 0s - loss: 0.0760 - val_loss: 0.5557\n",
      "Epoch 74/300\n",
      "379/379 [==============================] - 0s - loss: 0.0939 - val_loss: 0.3663\n",
      "Epoch 75/300\n",
      "379/379 [==============================] - 0s - loss: 0.0861 - val_loss: 0.2430\n",
      "Epoch 76/300\n",
      "379/379 [==============================] - 0s - loss: 0.0780 - val_loss: 0.4705\n",
      "Epoch 77/300\n",
      "379/379 [==============================] - 0s - loss: 0.0556 - val_loss: 0.2385\n",
      "Epoch 78/300\n",
      "379/379 [==============================] - 0s - loss: 0.0940 - val_loss: 0.3795\n",
      "Epoch 79/300\n",
      "379/379 [==============================] - 0s - loss: 0.0500 - val_loss: 0.2307\n",
      "Epoch 80/300\n",
      "379/379 [==============================] - 0s - loss: 0.0666 - val_loss: 0.2572\n",
      "Epoch 81/300\n",
      "379/379 [==============================] - 0s - loss: 0.0498 - val_loss: 0.3658\n",
      "Epoch 82/300\n",
      "379/379 [==============================] - 0s - loss: 0.1549 - val_loss: 0.2740\n",
      "Epoch 83/300\n",
      "379/379 [==============================] - 0s - loss: 0.2806 - val_loss: 0.2386\n",
      "Epoch 84/300\n",
      "379/379 [==============================] - 0s - loss: 0.1045 - val_loss: 0.2820\n",
      "Epoch 85/300\n",
      "379/379 [==============================] - 0s - loss: 0.1697 - val_loss: 0.6033\n",
      "Epoch 86/300\n",
      "379/379 [==============================] - 0s - loss: 1.3162 - val_loss: 0.2206\n",
      "Epoch 87/300\n",
      "379/379 [==============================] - 0s - loss: 0.1212 - val_loss: 0.1750\n",
      "Epoch 88/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.0573 - val_loss: 0.3247\n",
      "Epoch 89/300\n",
      "379/379 [==============================] - 0s - loss: 0.0579 - val_loss: 0.5111\n",
      "Epoch 90/300\n",
      "379/379 [==============================] - 0s - loss: 0.1227 - val_loss: 0.1344\n",
      "Epoch 91/300\n",
      "379/379 [==============================] - 0s - loss: 0.4668 - val_loss: 0.2764\n",
      "Epoch 92/300\n",
      "379/379 [==============================] - 0s - loss: 0.0621 - val_loss: 0.4072\n",
      "Epoch 93/300\n",
      "379/379 [==============================] - 0s - loss: 0.1783 - val_loss: 0.4343\n",
      "Epoch 94/300\n",
      "379/379 [==============================] - 0s - loss: 0.0715 - val_loss: 0.4008\n",
      "Epoch 95/300\n",
      "379/379 [==============================] - 0s - loss: 0.0742 - val_loss: 0.3089\n",
      "Epoch 96/300\n",
      "379/379 [==============================] - 0s - loss: 0.0427 - val_loss: 0.4466\n",
      "Epoch 97/300\n",
      "379/379 [==============================] - 0s - loss: 0.0583 - val_loss: 0.4257\n",
      "Epoch 98/300\n",
      "379/379 [==============================] - 0s - loss: 0.0978 - val_loss: 0.3435\n",
      "Epoch 99/300\n",
      "379/379 [==============================] - 0s - loss: 0.1798 - val_loss: 0.3870\n",
      "Epoch 100/300\n",
      "379/379 [==============================] - 0s - loss: 0.0424 - val_loss: 0.2244\n",
      "Epoch 101/300\n",
      "379/379 [==============================] - 0s - loss: 0.0675 - val_loss: 0.2285\n",
      "Epoch 102/300\n",
      "379/379 [==============================] - 0s - loss: 0.1500 - val_loss: 0.3678\n",
      "Epoch 103/300\n",
      "379/379 [==============================] - 0s - loss: 0.1081 - val_loss: 0.2667\n",
      "Epoch 104/300\n",
      "379/379 [==============================] - 0s - loss: 0.0317 - val_loss: 0.3453\n",
      "Epoch 105/300\n",
      "379/379 [==============================] - 0s - loss: 0.0665 - val_loss: 0.6239\n",
      "Epoch 106/300\n",
      "379/379 [==============================] - 0s - loss: 0.6757 - val_loss: 0.4447\n",
      "Epoch 107/300\n",
      "379/379 [==============================] - 0s - loss: 0.0954 - val_loss: 0.2936\n",
      "Epoch 108/300\n",
      "379/379 [==============================] - 0s - loss: 0.0356 - val_loss: 0.2549\n",
      "Epoch 109/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.030 - 0s - loss: 0.0266 - val_loss: 0.2262\n",
      "Epoch 110/300\n",
      "379/379 [==============================] - 0s - loss: 0.0276 - val_loss: 0.3366\n",
      "Epoch 111/300\n",
      "379/379 [==============================] - 0s - loss: 0.0576 - val_loss: 0.3366\n",
      "Epoch 112/300\n",
      "379/379 [==============================] - 0s - loss: 0.0353 - val_loss: 0.2514\n",
      "Epoch 113/300\n",
      "379/379 [==============================] - 0s - loss: 0.0264 - val_loss: 0.2198\n",
      "Epoch 114/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.029 - 0s - loss: 0.0635 - val_loss: 0.2006\n",
      "Epoch 115/300\n",
      "379/379 [==============================] - 0s - loss: 0.0422 - val_loss: 0.2472\n",
      "Epoch 116/300\n",
      "379/379 [==============================] - 0s - loss: 0.0250 - val_loss: 0.2505\n",
      "Epoch 117/300\n",
      "379/379 [==============================] - 0s - loss: 0.0247 - val_loss: 0.2532\n",
      "Epoch 118/300\n",
      "379/379 [==============================] - 0s - loss: 0.0693 - val_loss: 0.3236\n",
      "Epoch 119/300\n",
      "379/379 [==============================] - 0s - loss: 0.0832 - val_loss: 0.5914\n",
      "Epoch 120/300\n",
      "379/379 [==============================] - 0s - loss: 0.3256 - val_loss: 0.3135\n",
      "Epoch 121/300\n",
      "379/379 [==============================] - 0s - loss: 0.0285 - val_loss: 0.3021\n",
      "Epoch 122/300\n",
      "379/379 [==============================] - 0s - loss: 0.0347 - val_loss: 0.4469\n",
      "Epoch 123/300\n",
      "379/379 [==============================] - 0s - loss: 0.2501 - val_loss: 0.2488\n",
      "Epoch 124/300\n",
      "379/379 [==============================] - 0s - loss: 0.0258 - val_loss: 0.2687\n",
      "Epoch 125/300\n",
      "379/379 [==============================] - 0s - loss: 0.0211 - val_loss: 0.2529\n",
      "Epoch 126/300\n",
      "379/379 [==============================] - 0s - loss: 0.0296 - val_loss: 0.2104\n",
      "Epoch 127/300\n",
      "379/379 [==============================] - 0s - loss: 0.0848 - val_loss: 0.1558\n",
      "Epoch 128/300\n",
      "379/379 [==============================] - 0s - loss: 0.0380 - val_loss: 0.1896\n",
      "Epoch 129/300\n",
      "379/379 [==============================] - 0s - loss: 0.1401 - val_loss: 0.2202\n",
      "Epoch 130/300\n",
      "379/379 [==============================] - 0s - loss: 0.0466 - val_loss: 0.3972\n",
      "Epoch 131/300\n",
      "379/379 [==============================] - 0s - loss: 0.0539 - val_loss: 0.4369\n",
      "Epoch 132/300\n",
      "379/379 [==============================] - 0s - loss: 0.0837 - val_loss: 0.4928\n",
      "Epoch 133/300\n",
      "379/379 [==============================] - 0s - loss: 0.0368 - val_loss: 0.1786\n",
      "Epoch 134/300\n",
      "379/379 [==============================] - 0s - loss: 0.0532 - val_loss: 0.2378\n",
      "Epoch 135/300\n",
      "379/379 [==============================] - 0s - loss: 0.0264 - val_loss: 0.2181\n",
      "Epoch 136/300\n",
      "379/379 [==============================] - 0s - loss: 0.0218 - val_loss: 0.3038\n",
      "Epoch 137/300\n",
      "379/379 [==============================] - 0s - loss: 0.0254 - val_loss: 0.3536\n",
      "Epoch 138/300\n",
      "379/379 [==============================] - 0s - loss: 0.0954 - val_loss: 0.4492\n",
      "Epoch 139/300\n",
      "379/379 [==============================] - 0s - loss: 0.2607 - val_loss: 0.3348\n",
      "Epoch 140/300\n",
      "379/379 [==============================] - 0s - loss: 0.1365 - val_loss: 0.5443\n",
      "Epoch 141/300\n",
      "379/379 [==============================] - 0s - loss: 0.0611 - val_loss: 0.2242\n",
      "Epoch 142/300\n",
      "379/379 [==============================] - 0s - loss: 0.0260 - val_loss: 0.2157\n",
      "Epoch 143/300\n",
      "379/379 [==============================] - 0s - loss: 0.0215 - val_loss: 0.2362\n",
      "Epoch 144/300\n",
      "379/379 [==============================] - 0s - loss: 0.0258 - val_loss: 0.2313\n",
      "Epoch 145/300\n",
      "379/379 [==============================] - 0s - loss: 0.0232 - val_loss: 0.2196\n",
      "Epoch 146/300\n",
      "379/379 [==============================] - 0s - loss: 0.0181 - val_loss: 0.1370\n",
      "Epoch 147/300\n",
      "379/379 [==============================] - 0s - loss: 0.0386 - val_loss: 0.3476\n",
      "Epoch 148/300\n",
      "379/379 [==============================] - 0s - loss: 0.0353 - val_loss: 0.3829\n",
      "Epoch 149/300\n",
      "379/379 [==============================] - 0s - loss: 0.0339 - val_loss: 0.2187\n",
      "Epoch 150/300\n",
      "379/379 [==============================] - 0s - loss: 0.0151 - val_loss: 0.2375\n",
      "Epoch 151/300\n",
      "379/379 [==============================] - 0s - loss: 0.0190 - val_loss: 0.1829\n",
      "Epoch 152/300\n",
      "379/379 [==============================] - 0s - loss: 0.0815 - val_loss: 0.1209\n",
      "Epoch 153/300\n",
      "379/379 [==============================] - 0s - loss: 0.0279 - val_loss: 0.2382\n",
      "Epoch 154/300\n",
      "379/379 [==============================] - 0s - loss: 0.0191 - val_loss: 0.2050\n",
      "Epoch 155/300\n",
      "379/379 [==============================] - 0s - loss: 0.0145 - val_loss: 0.3233\n",
      "Epoch 156/300\n",
      "379/379 [==============================] - 0s - loss: 0.1551 - val_loss: 0.2785\n",
      "Epoch 157/300\n",
      "379/379 [==============================] - 0s - loss: 0.0460 - val_loss: 0.3514\n",
      "Epoch 158/300\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.037 - 0s - loss: 0.0473 - val_loss: 0.3215\n",
      "Epoch 159/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.1571\n",
      "Epoch 160/300\n",
      "379/379 [==============================] - 0s - loss: 0.0327 - val_loss: 0.1512\n",
      "Epoch 161/300\n",
      "379/379 [==============================] - 0s - loss: 0.2487 - val_loss: 0.1503\n",
      "Epoch 162/300\n",
      "379/379 [==============================] - 0s - loss: 0.0699 - val_loss: 0.2124\n",
      "Epoch 163/300\n",
      "379/379 [==============================] - 0s - loss: 0.0245 - val_loss: 0.1808\n",
      "Epoch 164/300\n",
      "379/379 [==============================] - 0s - loss: 0.0176 - val_loss: 0.2237\n",
      "Epoch 165/300\n",
      "379/379 [==============================] - 0s - loss: 0.0289 - val_loss: 0.1678\n",
      "Epoch 166/300\n",
      "379/379 [==============================] - 0s - loss: 0.1829 - val_loss: 0.1690\n",
      "Epoch 167/300\n",
      "379/379 [==============================] - 0s - loss: 0.0274 - val_loss: 0.1789\n",
      "Epoch 168/300\n",
      "379/379 [==============================] - 0s - loss: 0.0221 - val_loss: 0.4148\n",
      "Epoch 169/300\n",
      "379/379 [==============================] - 0s - loss: 0.0345 - val_loss: 0.2370\n",
      "Epoch 170/300\n",
      "379/379 [==============================] - 0s - loss: 0.0126 - val_loss: 0.2189\n",
      "Epoch 171/300\n",
      "379/379 [==============================] - 0s - loss: 0.0415 - val_loss: 0.1892\n",
      "Epoch 172/300\n",
      "379/379 [==============================] - 0s - loss: 0.0160 - val_loss: 0.2507\n",
      "Epoch 173/300\n",
      "379/379 [==============================] - 0s - loss: 0.0139 - val_loss: 0.2643\n",
      "Epoch 174/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.0619 - val_loss: 0.3175\n",
      "Epoch 175/300\n",
      "379/379 [==============================] - 0s - loss: 0.0381 - val_loss: 0.2114\n",
      "Epoch 176/300\n",
      "379/379 [==============================] - 0s - loss: 0.0365 - val_loss: 0.1871\n",
      "Epoch 177/300\n",
      "379/379 [==============================] - 0s - loss: 0.0367 - val_loss: 0.2415\n",
      "Epoch 178/300\n",
      "379/379 [==============================] - 0s - loss: 0.0244 - val_loss: 0.2317\n",
      "Epoch 179/300\n",
      "379/379 [==============================] - 0s - loss: 0.0188 - val_loss: 0.3519\n",
      "Epoch 180/300\n",
      "379/379 [==============================] - 0s - loss: 0.0463 - val_loss: 0.4272\n",
      "Epoch 181/300\n",
      "379/379 [==============================] - 0s - loss: 0.1017 - val_loss: 0.2510\n",
      "Epoch 182/300\n",
      "379/379 [==============================] - 0s - loss: 0.0146 - val_loss: 0.2187\n",
      "Epoch 183/300\n",
      "379/379 [==============================] - 0s - loss: 0.0155 - val_loss: 0.2911\n",
      "Epoch 184/300\n",
      "379/379 [==============================] - 0s - loss: 0.1442 - val_loss: 0.5659\n",
      "Epoch 185/300\n",
      "379/379 [==============================] - 0s - loss: 0.0455 - val_loss: 0.2486\n",
      "Epoch 186/300\n",
      "379/379 [==============================] - 0s - loss: 0.0145 - val_loss: 0.3361\n",
      "Epoch 187/300\n",
      "379/379 [==============================] - 0s - loss: 0.0391 - val_loss: 0.2304\n",
      "Epoch 188/300\n",
      "379/379 [==============================] - 0s - loss: 0.0152 - val_loss: 0.2176\n",
      "Epoch 189/300\n",
      "379/379 [==============================] - 0s - loss: 0.0162 - val_loss: 0.2340\n",
      "Epoch 190/300\n",
      "379/379 [==============================] - 0s - loss: 0.0106 - val_loss: 0.2327\n",
      "Epoch 191/300\n",
      "379/379 [==============================] - 0s - loss: 0.0222 - val_loss: 0.2839\n",
      "Epoch 192/300\n",
      "379/379 [==============================] - 0s - loss: 0.0152 - val_loss: 0.1823\n",
      "Epoch 193/300\n",
      "379/379 [==============================] - 0s - loss: 0.0296 - val_loss: 0.1949\n",
      "Epoch 194/300\n",
      "379/379 [==============================] - 0s - loss: 0.0381 - val_loss: 0.2426\n",
      "Epoch 195/300\n",
      "379/379 [==============================] - 0s - loss: 0.0128 - val_loss: 0.1865\n",
      "Epoch 196/300\n",
      "379/379 [==============================] - 0s - loss: 0.0107 - val_loss: 0.1882\n",
      "Epoch 197/300\n",
      "379/379 [==============================] - 0s - loss: 0.0143 - val_loss: 0.2498\n",
      "Epoch 198/300\n",
      "379/379 [==============================] - 0s - loss: 0.0149 - val_loss: 0.2537\n",
      "Epoch 199/300\n",
      "379/379 [==============================] - 0s - loss: 0.0339 - val_loss: 0.3026\n",
      "Epoch 200/300\n",
      "379/379 [==============================] - 0s - loss: 0.0214 - val_loss: 0.2712\n",
      "Epoch 201/300\n",
      "379/379 [==============================] - 0s - loss: 0.0389 - val_loss: 0.1761\n",
      "Epoch 202/300\n",
      "379/379 [==============================] - 0s - loss: 0.0196 - val_loss: 0.2735\n",
      "Epoch 203/300\n",
      "379/379 [==============================] - 0s - loss: 0.0523 - val_loss: 0.3530\n",
      "Epoch 204/300\n",
      "379/379 [==============================] - 0s - loss: 0.0248 - val_loss: 0.3093\n",
      "Epoch 205/300\n",
      "379/379 [==============================] - 0s - loss: 0.0178 - val_loss: 0.1906\n",
      "Epoch 206/300\n",
      "379/379 [==============================] - 0s - loss: 0.0198 - val_loss: 0.2168\n",
      "Epoch 207/300\n",
      "379/379 [==============================] - 0s - loss: 0.0092 - val_loss: 0.2167\n",
      "Epoch 208/300\n",
      "379/379 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2211\n",
      "Epoch 209/300\n",
      "379/379 [==============================] - 0s - loss: 0.0148 - val_loss: 0.1990\n",
      "Epoch 210/300\n",
      "379/379 [==============================] - 0s - loss: 0.0186 - val_loss: 0.2565\n",
      "Epoch 211/300\n",
      "379/379 [==============================] - 0s - loss: 0.0117 - val_loss: 0.2273\n",
      "Epoch 212/300\n",
      "379/379 [==============================] - 0s - loss: 0.0134 - val_loss: 0.2069\n",
      "Epoch 213/300\n",
      "379/379 [==============================] - 0s - loss: 0.0270 - val_loss: 0.1932\n",
      "Epoch 214/300\n",
      "379/379 [==============================] - 0s - loss: 0.0221 - val_loss: 0.1576\n",
      "Epoch 215/300\n",
      "379/379 [==============================] - 0s - loss: 0.0312 - val_loss: 0.1375\n",
      "Epoch 216/300\n",
      "379/379 [==============================] - 0s - loss: 0.0193 - val_loss: 0.2264\n",
      "Epoch 217/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2167\n",
      "Epoch 218/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2225\n",
      "Epoch 219/300\n",
      "379/379 [==============================] - 0s - loss: 0.0084 - val_loss: 0.2204\n",
      "Epoch 220/300\n",
      "379/379 [==============================] - 0s - loss: 0.0100 - val_loss: 0.3408\n",
      "Epoch 221/300\n",
      "379/379 [==============================] - 0s - loss: 0.0154 - val_loss: 0.3708\n",
      "Epoch 222/300\n",
      "379/379 [==============================] - 0s - loss: 0.0564 - val_loss: 0.2979\n",
      "Epoch 223/300\n",
      "379/379 [==============================] - 0s - loss: 0.0113 - val_loss: 0.1977\n",
      "Epoch 224/300\n",
      "379/379 [==============================] - 0s - loss: 0.0362 - val_loss: 0.2174\n",
      "Epoch 225/300\n",
      "379/379 [==============================] - 0s - loss: 0.0205 - val_loss: 0.2274\n",
      "Epoch 226/300\n",
      "379/379 [==============================] - 0s - loss: 0.0136 - val_loss: 0.2090\n",
      "Epoch 227/300\n",
      "379/379 [==============================] - 0s - loss: 0.0081 - val_loss: 0.2326\n",
      "Epoch 228/300\n",
      "379/379 [==============================] - 0s - loss: 0.0185 - val_loss: 0.2345\n",
      "Epoch 229/300\n",
      "379/379 [==============================] - 0s - loss: 0.0339 - val_loss: 0.3611\n",
      "Epoch 230/300\n",
      "379/379 [==============================] - 0s - loss: 0.0322 - val_loss: 0.2440\n",
      "Epoch 231/300\n",
      "379/379 [==============================] - 0s - loss: 0.0142 - val_loss: 0.2736\n",
      "Epoch 232/300\n",
      "379/379 [==============================] - 0s - loss: 0.0778 - val_loss: 0.2740\n",
      "Epoch 233/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.1981\n",
      "Epoch 234/300\n",
      "379/379 [==============================] - 0s - loss: 0.0189 - val_loss: 0.1878\n",
      "Epoch 235/300\n",
      "379/379 [==============================] - 0s - loss: 0.0814 - val_loss: 0.1310\n",
      "Epoch 236/300\n",
      "379/379 [==============================] - 0s - loss: 0.0158 - val_loss: 0.2619\n",
      "Epoch 237/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.1690\n",
      "Epoch 238/300\n",
      "379/379 [==============================] - 0s - loss: 0.0134 - val_loss: 0.2277\n",
      "Epoch 239/300\n",
      "379/379 [==============================] - 0s - loss: 0.0127 - val_loss: 0.2538\n",
      "Epoch 240/300\n",
      "379/379 [==============================] - 0s - loss: 0.0184 - val_loss: 0.2170\n",
      "Epoch 241/300\n",
      "379/379 [==============================] - 0s - loss: 0.0071 - val_loss: 0.1796\n",
      "Epoch 242/300\n",
      "379/379 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2088\n",
      "Epoch 243/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.3252\n",
      "Epoch 244/300\n",
      "379/379 [==============================] - 0s - loss: 0.0295 - val_loss: 0.2914\n",
      "Epoch 245/300\n",
      "379/379 [==============================] - 0s - loss: 0.0204 - val_loss: 0.2447\n",
      "Epoch 246/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.1837\n",
      "Epoch 247/300\n",
      "379/379 [==============================] - 0s - loss: 0.0147 - val_loss: 0.3304\n",
      "Epoch 248/300\n",
      "379/379 [==============================] - 0s - loss: 0.0157 - val_loss: 0.3002\n",
      "Epoch 249/300\n",
      "379/379 [==============================] - 0s - loss: 0.0191 - val_loss: 0.2288\n",
      "Epoch 250/300\n",
      "379/379 [==============================] - 0s - loss: 0.0093 - val_loss: 0.2854\n",
      "Epoch 251/300\n",
      "379/379 [==============================] - 0s - loss: 0.0080 - val_loss: 0.1949\n",
      "Epoch 252/300\n",
      "379/379 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2106\n",
      "Epoch 253/300\n",
      "379/379 [==============================] - 0s - loss: 0.0251 - val_loss: 0.2714\n",
      "Epoch 254/300\n",
      "379/379 [==============================] - 0s - loss: 0.0323 - val_loss: 0.1994\n",
      "Epoch 255/300\n",
      "379/379 [==============================] - 0s - loss: 0.0102 - val_loss: 0.2318\n",
      "Epoch 256/300\n",
      "379/379 [==============================] - 0s - loss: 0.0097 - val_loss: 0.2061\n",
      "Epoch 257/300\n",
      "379/379 [==============================] - 0s - loss: 0.0082 - val_loss: 0.2566\n",
      "Epoch 258/300\n",
      "379/379 [==============================] - 0s - loss: 0.0077 - val_loss: 0.2393\n",
      "Epoch 259/300\n",
      "379/379 [==============================] - 0s - loss: 0.0076 - val_loss: 0.2252\n",
      "Epoch 260/300\n",
      "379/379 [==============================] - 0s - loss: 0.0064 - val_loss: 0.1965\n",
      "Epoch 261/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s - loss: 0.0106 - val_loss: 0.1587\n",
      "Epoch 262/300\n",
      "379/379 [==============================] - 0s - loss: 0.0138 - val_loss: 0.2070\n",
      "Epoch 263/300\n",
      "379/379 [==============================] - 0s - loss: 0.0172 - val_loss: 0.2044\n",
      "Epoch 264/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.2525\n",
      "Epoch 265/300\n",
      "379/379 [==============================] - 0s - loss: 0.0094 - val_loss: 0.2126\n",
      "Epoch 266/300\n",
      "379/379 [==============================] - 0s - loss: 0.0065 - val_loss: 0.1847\n",
      "Epoch 267/300\n",
      "379/379 [==============================] - 0s - loss: 0.0075 - val_loss: 0.2186\n",
      "Epoch 268/300\n",
      "379/379 [==============================] - 0s - loss: 0.0061 - val_loss: 0.2013\n",
      "Epoch 269/300\n",
      "379/379 [==============================] - 0s - loss: 0.0079 - val_loss: 0.2289\n",
      "Epoch 270/300\n",
      "379/379 [==============================] - 0s - loss: 0.0066 - val_loss: 0.2070\n",
      "Epoch 271/300\n",
      "379/379 [==============================] - 0s - loss: 0.0188 - val_loss: 0.1638\n",
      "Epoch 272/300\n",
      "379/379 [==============================] - 0s - loss: 0.0166 - val_loss: 0.1678\n",
      "Epoch 273/300\n",
      "379/379 [==============================] - 0s - loss: 0.0182 - val_loss: 0.1386\n",
      "Epoch 274/300\n",
      "379/379 [==============================] - 0s - loss: 0.0256 - val_loss: 0.1649\n",
      "Epoch 275/300\n",
      "379/379 [==============================] - 0s - loss: 0.0162 - val_loss: 0.1624\n",
      "Epoch 276/300\n",
      "379/379 [==============================] - 0s - loss: 0.0333 - val_loss: 0.2056\n",
      "Epoch 277/300\n",
      "379/379 [==============================] - 0s - loss: 0.0137 - val_loss: 0.2138\n",
      "Epoch 278/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.2276\n",
      "Epoch 279/300\n",
      "379/379 [==============================] - 0s - loss: 0.0114 - val_loss: 0.2439\n",
      "Epoch 280/300\n",
      "379/379 [==============================] - 0s - loss: 0.0107 - val_loss: 0.2611\n",
      "Epoch 281/300\n",
      "379/379 [==============================] - 0s - loss: 0.0090 - val_loss: 0.1657\n",
      "Epoch 282/300\n",
      "379/379 [==============================] - 0s - loss: 0.0347 - val_loss: 0.1560\n",
      "Epoch 283/300\n",
      "379/379 [==============================] - 0s - loss: 0.0680 - val_loss: 0.1518\n",
      "Epoch 284/300\n",
      "379/379 [==============================] - 0s - loss: 0.2871 - val_loss: 0.0956\n",
      "Epoch 285/300\n",
      "379/379 [==============================] - 0s - loss: 0.1493 - val_loss: 0.1698\n",
      "Epoch 286/300\n",
      "379/379 [==============================] - 0s - loss: 0.0112 - val_loss: 0.2153\n",
      "Epoch 287/300\n",
      "379/379 [==============================] - 0s - loss: 0.0062 - val_loss: 0.1987\n",
      "Epoch 288/300\n",
      "379/379 [==============================] - 0s - loss: 0.0078 - val_loss: 0.1703\n",
      "Epoch 289/300\n",
      "379/379 [==============================] - 0s - loss: 0.0080 - val_loss: 0.2966\n",
      "Epoch 290/300\n",
      "379/379 [==============================] - 0s - loss: 0.0165 - val_loss: 0.3162\n",
      "Epoch 291/300\n",
      "379/379 [==============================] - 0s - loss: 0.0079 - val_loss: 0.1729\n",
      "Epoch 292/300\n",
      "379/379 [==============================] - 0s - loss: 0.0167 - val_loss: 0.1548\n",
      "Epoch 293/300\n",
      "379/379 [==============================] - 0s - loss: 0.0083 - val_loss: 0.2077\n",
      "Epoch 294/300\n",
      "379/379 [==============================] - 0s - loss: 0.0073 - val_loss: 0.2451\n",
      "Epoch 295/300\n",
      "379/379 [==============================] - 0s - loss: 0.0088 - val_loss: 0.2492\n",
      "Epoch 296/300\n",
      "379/379 [==============================] - 0s - loss: 0.0087 - val_loss: 0.2318\n",
      "Epoch 297/300\n",
      "379/379 [==============================] - 0s - loss: 0.0061 - val_loss: 0.2132\n",
      "Epoch 298/300\n",
      "379/379 [==============================] - 0s - loss: 0.0070 - val_loss: 0.1588\n",
      "Epoch 299/300\n",
      "379/379 [==============================] - 0s - loss: 0.0451 - val_loss: 0.1638\n",
      "Epoch 300/300\n",
      "379/379 [==============================] - 0s - loss: 0.0266 - val_loss: 0.1751\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEKCAYAAAAVaT4rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhMH8f7EPPR1Qt8CWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcba917be90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1, init='uniform'))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "hist = model.fit(X_train_scaled.as_matrix(), y_train.as_matrix(), nb_epoch=300,\n",
    "verbose=1, validation_data=(X_test_scaled.as_matrix(), y_test.as_matrix()))\n",
    "\n",
    "\n",
    "listaloss2 = []\n",
    "                \n",
    "for i in range(300):\n",
    "    listaloss2.append(hist.history['loss'][i])\n",
    "\n",
    "plt.plot(range(301)[1:],listaloss,'g',label='sigmoid')\n",
    "plt.plot(range(301)[1:],listaloss2,'y',label='relu')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='uper right')\n",
    "plt.axis([0,50,0,5])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) repetir (c) y (d) variando tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "\n",
    "\n",
    "for lr in lear_rate:\n",
    "    \n",
    "    #sigmoid\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    sgd = SGD(lr=0.01)\n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "    hist = model.fit(X_train_scaled.as_matrix(), y_train.as_matrix(), nb_epoch=300,\n",
    "    verbose = 1, validation_data=(X_test_scaled.as_matrix(), y_test.as_matrix()))\n",
    "    listaloss = []\n",
    "                \n",
    "    for i in range(300):\n",
    "        listaloss.append(hist.history['loss'][i])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #relu\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=X_train_scaled.shape[1], init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    sgd = SGD(lr=lr)\n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "\n",
    "    hist = model.fit(X_train_scaled.as_matrix(), y_train.as_matrix(), nb_epoch=300,\n",
    "    verbose=1, validation_data=(X_test_scaled.as_matrix(), y_test.as_matrix()))\n",
    "\n",
    "\n",
    "    listaloss2 = []\n",
    "                \n",
    "    for i in range(300):\n",
    "        listaloss2.append(hist.history['loss'][i])\n",
    "\n",
    "    plt.plot(range(301)[1:],listaloss,'g',label='sigmoid')\n",
    "    plt.plot(range(301)[1:],listaloss2,'y',label='relu')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(loc='uper right')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "Xm = X_train_scaled.as_matrix()\n",
    "ym = y_train.as_matrix()\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    # create model\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(200, input_dim=Xm.shape[1], init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.2)\n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    # Fit the model\n",
    "    model.fit(Xm[train], ym[train], nb_epoch=300)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_decay = 21\n",
    "momentum = np.linspace(0,1,n_decay)\n",
    "sgd = SGD(lr=0.2,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled.as_matrix(),y_train_scaled.as_matrix(),batch_size=50,nb_epoch=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "moptimizer = Adagrad(lr=0.01)\n",
    "model.compile(optimizer=moptimizer)\n",
    "model.fit(X_train_scaled.as_matrix(),y_train_scaled.as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#la regularization se debe incorporar a cada capa separadamente\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(200,input_dim=idim,init='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, init='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2))\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(200,input_dim=idim,init='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, init='uniform'))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
